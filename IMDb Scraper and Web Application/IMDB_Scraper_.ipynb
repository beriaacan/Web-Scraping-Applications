{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP10OfvanN6jZKoXVOItJhR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/beriaacan/Web-Scraping-Applications/blob/main/IMDb%20Scraper%20and%20Web%20Application/IMDB_Scraper_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we will explore web scraping techniques to gather data from IMDb's website, focusing on the top 1000 movies as rated by users. We will use the BeautifulSoup library in Python to extract movie details such as the movie name, certificate rating, duration, genre, IMDb rating, metascore, director, stars, votes, gross earnings, and a brief plot summary.\n",
        "\n",
        "\n",
        "The goal of this project is to collect valuable data from IMDb, which can be used for various analyses, recommendations, and insights. We will perform web scraping on multiple pages to gather comprehensive information about these top-rated movies. The final dataset will be used for further analysis and visualization."
      ],
      "metadata": {
        "id": "f9vpDcLRsYF_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Installing required libraries"
      ],
      "metadata": {
        "id": "nzlsJdv4mGtG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Requests: This library allows us to make HTTP requests to web pages, which is crucial for fetching web content.\n",
        "\n",
        "Beautiful Soup (beautifulsoup4): This is a powerful library for web scraping. It helps parse and extract data from HTML and XML documents.\n",
        "\n",
        "We need these packages to scrape data from a website successfully. If you haven't installed these packages yet, you can run the following commands:"
      ],
      "metadata": {
        "id": "JED_vNGSs9TZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This mounts your Google Drive to the Colab VM.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZ6d74TBvXlk",
        "outputId": "5ff2f466-174b-44d4-9133-baa44a8c0822"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FOLDERNAME ='deneme/web scraping'\n",
        "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
        "\n",
        "# Now that we've mounted your Drive, this ensures that\n",
        "# the Python interpreter of the Colab VM can load\n",
        "# python files from within it.\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))"
      ],
      "metadata": {
        "id": "_mzXBQgYvfrb"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFqWA5R1l6Vv",
        "outputId": "09b17e48-5512-46d7-912e-4ae0c9d4144b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MglRwaP-l86c",
        "outputId": "3bfdb5b0-a93d-4d02-f7b9-2a916f83370d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports libraries"
      ],
      "metadata": {
        "id": "X98_STZLmOJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "rn9sPf7Wo0P4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seaborn (sns): Seaborn is a data visualization library that works in conjunction with Matplotlib. It provides a high-level interface for creating informative and attractive statistical graphics.\n",
        "\n",
        "Requests: The 'requests' library enables making HTTP requests to fetch data from websites. It's essential for web scraping.\n",
        "\n",
        "Beautiful Soup (from bs4 import BeautifulSoup): Beautiful Soup is a library for web scraping. It allows you to parse and navigate HTML and XML documents, making it easier to extract data from web pages."
      ],
      "metadata": {
        "id": "7T2X64RntEOA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Web Scraping IMDb's Top 1000 Movies"
      ],
      "metadata": {
        "id": "Z-3AhOf5qyMU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this cell, we perform the initial steps of web scraping IMDb's top 1000 movies:\n",
        "\n",
        "URL Generation: We generate a list of URLs to access different pages of IMDb's top 1000 movies. IMDb lists movies in groups of 100, and we loop through 10 pages, each containing 100 movies. We use a base URL with parameters to specify the sorting and starting point.\n",
        "\n",
        "HTTP Requests: For each URL, we send an HTTP GET request using the 'requests' library to fetch the webpage's HTML content.\n",
        "\n",
        "Parsing with BeautifulSoup: We use BeautifulSoup to parse the HTML content of each page. BeautifulSoup allows us to navigate and extract specific information from the web pages."
      ],
      "metadata": {
        "id": "m4ezQbAntK54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = []   # Initialize an empty list to store URLs\n",
        "page = []  # Initialize an empty list to store webpage content\n",
        "soup = []  # Initialize an empty list to store BeautifulSoup objects"
      ],
      "metadata": {
        "id": "g5gRgY6ivLkP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, 10):\n",
        "    # Generate IMDb URLs for the top 1000 movies, adjusting the 'start' parameter\n",
        "    url.append(f\"https://www.imdb.com/search/title/?groups=top_1000&sort=user_rating,desc&count=100&start={100 * i + 1}&ref_=adv_nxt\")\n",
        "\n",
        "    # Send an HTTP GET request to fetch the webpage content\n",
        "    page.append(requests.get(url[i]))\n",
        "\n",
        "    # Parse the webpage content using BeautifulSoup with the \"html.parser\" parser\n",
        "    soup.append(BeautifulSoup(page[i].content, \"html.parser\"))"
      ],
      "metadata": {
        "id": "l2znoHbequOS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_top1000 = pd.DataFrame(columns=['Movie Name', 'Certificate', 'Duration', 'Genre', 'IMDb Rating', 'Metascore', 'Director', 'Stars', 'Votes', 'Grossed', 'Plot'])"
      ],
      "metadata": {
        "id": "qmeuPnpyq8Zt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Movie Name: The name of the movie.\n",
        "- Certificate: The age certificate or rating of the movie.\n",
        "- Duration: The duration of the movie.\n",
        "- Genre: The genre(s) of the movie.\n",
        "- IMDb Rating: The IMDb rating of the movie.\n",
        "- Metascore: The Metascore rating of the movie (if available).\n",
        "- Director: The director(s) of the movie.\n",
        "- Stars: The main cast or stars of the movie.\n",
        "- Votes: The number of votes/ratings the movie has received on IMDb.\n",
        "- Grossed: The total gross earnings of the movie (if available).\n",
        "- Plot: A brief plot summary of the movie."
      ],
      "metadata": {
        "id": "FUawZcgwtP44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):  # Loop through the 10 pages\n",
        "    # Find all the movie containers on the page\n",
        "    movie_containers = soup[i].find_all('div', class_='lister-item mode-advanced')\n",
        "\n",
        "    for container in movie_containers:\n",
        "        # Movie Name\n",
        "        name = container.h3.a.text.strip()\n",
        "\n",
        "        # Certificate\n",
        "        certificate = container.p.find('span', class_='certificate').text.strip() if container.p.find('span', class_='certificate') else ''\n",
        "\n",
        "        # Duration\n",
        "        duration = container.p.find('span', class_='runtime').text.strip() if container.p.find('span', class_='runtime') else ''\n",
        "\n",
        "        # Genre\n",
        "        genre = container.p.find('span', class_='genre').text.strip() if container.p.find('span', class_='genre') else ''\n",
        "\n",
        "        # IMDb Rating\n",
        "        imdb_rating = container.strong.text.strip()\n",
        "\n",
        "        # Metascore\n",
        "        metascore = container.find('span', class_='metascore').text.strip() if container.find('span', class_='metascore') else ''\n",
        "\n",
        "        # Director\n",
        "        director = container.find('p', class_='').find('a').text.strip() if container.find('p', class_='').find('a') else ''\n",
        "\n",
        "        # Stars\n",
        "        stars = [star.text.strip() for star in container.find_all('a', href=lambda href: href and \"st\" in href)]\n",
        "\n",
        "        # Votes\n",
        "        votes = container.find('span', attrs={'name': 'nv'}).text.strip().replace(',', '')\n",
        "\n",
        "        # Grossed\n",
        "        grossed = container.find_all('span', attrs={'name': 'nv'})[1].text.strip().replace(',', '') if len(container.find_all('span', attrs={'name': 'nv'})) > 1 else ''\n",
        "\n",
        "        # Plot\n",
        "        plot = container.find_all('p', class_='text-muted')[-1].text.strip() if container.find_all('p', class_='text-muted') else ''\n",
        "\n",
        "        # Append the data to the DataFrame\n",
        "        imdb_top1000 = imdb_top1000.append({\n",
        "            'Movie Name': name,\n",
        "            'Certificate': certificate,\n",
        "            'Duration': duration,\n",
        "            'Genre': genre,\n",
        "            'IMDb Rating': imdb_rating,\n",
        "            'Metascore': metascore,\n",
        "            'Director': director,\n",
        "            'Stars': ', '.join(stars),\n",
        "            'Votes': votes,\n",
        "            'Grossed': grossed,\n",
        "            'Plot': plot\n",
        "        }, ignore_index=True)\n"
      ],
      "metadata": {
        "id": "Mu3ADQNMwanI"
      },
      "execution_count": 25,
      "outputs": []
    }
  ]
}